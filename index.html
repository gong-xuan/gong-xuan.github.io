
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
    <meta http-equiv="content-type" content="text/html; charset=us-ascii">
    <meta name="viewport" content="&acirc;&#8364;&#339;width=800&acirc;&#8364;&#157;">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 18px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
    .auto-style1 {
		color: #1772D0;
	}
  .auto-style2 {
	text-align: right;
}
    .auto-style3 {
		color: #800000;
	}
    .auto-style4 {
		color: #FF0000;
	}
  </style>
    <link rel="icon" type="image/png" href="seal_icon.png">
    <title>Xuan Gong' Homepage</title>
    <link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic"
      rel="stylesheet"
      type="text/css">
  </head>
  <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-E97EGMN96G"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-E97EGMN96G');
</script>
  <body>
    <table width="800" cellspacing="0" cellpadding="0" border="0" align="center">
      <tbody>
        <tr>
          <td>
            <table width="100%" cellspacing="0" cellpadding="20" border="0" align="center">
              <tbody>
              <tr>
                  <td width="75%" valign="middle">
                    <p align="center"> <name>Xuan Gong </name> </p>
                    <p align="center">
                    <b>Ph.D. student</b><br>
                        Department of Computer Science and Engineering, University at Buffalo<br>
                        Email: xuangong AT buffalo DOT edu<br> 
                    <br>
                    </p>
                    <p align="center">
                    <a href="https://scholar.google.com/citations?user=sTqQ-jgAAAAJ&hl=en&oi=ao">Google scholar</a> /
                    <a href="https://www.linkedin.com/in/xuan-gong-92ab59238/">LinkedIn</a> / 
                    <a href="./pdf/CV_xuan.pdf">CV</a> 
                    </p>
                  </td>
                  <td width="25%"> 
				  <img src="./pdf/XG.png" height="120" width="120"> </td>
                </tr>
              </tbody>
            </table>
            <table width="100%" cellspacing="0" cellpadding="20" border="0" align="center">
                <tbody>
                    <tr>
                      <td width="100%" valign="middle"> <heading>Bio</heading>
                      <p>  I am a final-year Ph.D. student advised by Prof. <a href="https://cse.buffalo.edu/~doermann">David Doermann</a>. 
                    I got bachelor and master degree from Beihang University. <br>
                    I worked as a Research Intern at 
                    <a href="https://tech.fb.com/ar-vr/">Meta Reality Lab</a>, 
                    <a href="https://www.innopeaktech.com/research">OPPO US Research</a>, and <a href="https://www.uii-ai.com/en">UII America</a>.
                    </p>
                      </td>
                    </tr>
                  </tbody>
                  <tr>
                    <td width="100%" valign="middle"> <heading>Current Reseach</heading>
                        <p>  
                            <li><b>3D vision</b>: human mesh reconstruction, neural radiance fields</li>
                            <li><b>Medical imaging</b>: cancer prognosis, image registration, endscopy video analysis</li>
                            <li><b>Federated learning</b>: federated ensemble distillation</li>
                      </p>
                    </td>
                </tr>
              <tbody>
                <tr>
                  <td width="100%" valign="middle"> <heading>Selected Publications</heading>
                  </td>
                </tr>
              </tbody>
            </table>
            <table width="100%" cellspacing="0" cellpadding="20" border="0" align="center">
              <tbody>
                       <tr>
                  <td width="25%">
                    <img src="./pdf/aaai23.png" height="141" width="158">
                    </td>
                  <td width="75%" valign="top">
                    <p>
					<a href="https://arxiv.org/pdf/2212.05223.pdf"><papertitle>
Progressive Multi-view Human Mesh Recovery with Self-Supervision</papertitle></a>
					<br>
                    <strong>Xuan Gong</strong>,
					<a href="https://lsongx.github.io/">Liangchen Song</a>, 
					<a href="https://mzhengrpi.github.io/">Meng Zheng</a>, 
                    <a href="https://planche.me/">Benjamin Planche</a>,&nbsp;
					<a href="https://scholar.google.com/citations?user=S2BT6ogAAAAJ&hl=en">Terrence Chen</a>,  
					<a href="https://cse.buffalo.edu/~jsyuan/">Junsong Yuan</a>, 
					<a href="https://cse.buffalo.edu/~doermann/">David Doermann</a>,
					<a href="http://wuziyan.com/"> Ziyan Wu</a> <br><em>AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>)</em>, 
					2023&nbsp; <span class="auto-style4"><strong>(oral)</strong></span><br>
					</p>
                    <p>We propose a novel simulation-based training pipeline for multi-view human mesh recovery, which (a) relies on intermediate 2D representations which are more robust to synthetic-to-real domain gap; (b) leverages learnable calibration and triangulation to adapt to more diversified camera setups; and (c) progressively aggregates multi-view information in a canonical 3D space to remove ambiguities in 2D representations. </p>
                  </td>
                  </tr>
				  
                     <tr>
                  <td width="25%">
                    <img src="./pdf/TMI2022.jpg" height="141" width="158">
                    </td>
                  <td width="75%" valign="top">
                    <p>
					<a href="https://arxiv.org/abs/2210.08464"><papertitle>
Federated Learning with Privacy-Preserving Ensemble Attention Distillation</papertitle></a>
					<br>
					<strong>Xuan Gong</strong>,
					<a href="https://lsongx.github.io/">Liangchen Song</a>,
					<a href="https://www.linkedin.com/in/rishivedula">Rishi Vedula</a>,
					<a href="https://www.linkedin.com/in/abhi5heksharma">Abhishek Sharma</a>, 
					<a href="https://mzhengrpi.github.io/">Meng Zheng</a>, 
                    <a href="https://planche.me/">Benjamin Planche</a>,&nbsp;
					<a href="https://www.linkedin.com/in/arun-innanje">Arun Innanje</a>,   
					<a href="https://scholar.google.com/citations?user=S2BT6ogAAAAJ&hl=en">Terrence Chen</a>, 
					<a href="https://cse.buffalo.edu/~jsyuan/">Junsong Yuan</a>, 
					<a href="https://cse.buffalo.edu/~doermann/">David Doermann</a>,
					<a href="http://wuziyan.com/">Ziyan Wu</a> <br>
                      <em>IEEE Transactions on Medical Imaging (<strong>TMI</strong>)</em>, 2022<br>
					</p>
                    <p>We propose a privacy-preserving FL framework leveraging unlabeled public data for one-way offline knowledge distillation in this work. The central model is learned from local knowledge via ensemble attention distillation. Our technique uses decentralized and heterogeneous local data like existing FL approaches, but more importantly, it significantly reduces the risk of privacy leakage. We demonstrate that our method achieves very competitive performance with more robust privacy preservation based on extensive experiments on image classification, segmentation, and reconstruction tasks.</p>
                  </td>
                  </tr>
              
                  <tr>
                    <td width="25%">
                      <img src="./pdf/cra.png" height="141" width="158">
                      </td>
                    <td width="75%" valign="top">
                      <p>
                      <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136610210.pdf"><papertitle>
  Self-supervised Human Mesh Recovery with Cross-Representation Alignment</papertitle></a>
                      <br>
                      <strong>Xuan Gong</strong>, 
                      <a href="https://mzhengrpi.github.io/">Meng Zheng</a>, 
                      <a href="https://planche.me/">Benjamin Planche</a>, 
                      <a href="https://karanams.github.io/">Srikrishna Karanam</a>,
                      <a href="https://scholar.google.com/citations?user=S2BT6ogAAAAJ&hl=en">Terrence Chen</a>,
                      <a href="https://cse.buffalo.edu/~doermann/">David Doermann</a>,
                      <a href="http://wuziyan.com/">Ziyan Wu</a>
                      <br>
                         <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 
                      2022<br>
                      </p>
                      

                      <p>We propose cross-representation alignment utilizing the complementary information from the robust but sparse representation (2D keypoints). Specifically, the alignment errors between initial mesh estimation and both 2D representations are forwarded into regressor and dynamically corrected in the following mesh regression. This adaptive cross-representation alignment explicitly learns from the deviations and captures complementary information: robustness from sparse representation and richness from dense representation. </p>
                    </td>
                                               </tr>

                   <tr>
                  <td width="25%">
                    <img src="./pdf/pref.png" height="141" width="158">
                    </td>
                  <td width="75%" valign="top">
                    <p>
					<a href="https://arxiv.org/pdf/2209.10691.pdf"><papertitle>
PREF: Predictability Regularized Neural Motion Fields</papertitle></a>
					<br>
					<a href="https://lsongx.github.io/">Liangchen Song</a>, 
					<strong>Xuan Gong</strong>, 
					<a href="https://planche.me/">Benjamin Planche</a>, 
                    <a href="https://mzhengrpi.github.io/">Meng Zheng</a>, 
                    <a href="https://cse.buffalo.edu/~doermann/">David Doermann</a>,
                    <a href="https://cse.buffalo.edu/~jsyuan/">Junsong Yuan</a>,
					<a href="https://scholar.google.com/citations?user=S2BT6ogAAAAJ&hl=en">Terrence Chen</a>,
					<a href="http://wuziyan.com/">Ziyan Wu</a> 
					<br>
                       <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 
					2022 <span class="auto-style4"><strong>(oral)</strong></span><br>
					&nbsp;[<a href="http://pref.uiius.com/">project page</a>]<br>
					</p>
                    <p>We leverage a neural motion field for estimating the motion of all points in a multiview setting. Modeling the motion from a dynamic scene with multiview data is challenging due to the ambiguities in points of similar color and points with time-varying color. We propose to regularize the estimated motion to be predictable. If the motion from previous frames is known, then the motion in the near future should be predictable. Therefore, we introduce a predictability regularization by first conditioning the estimated motion on latent embeddings, then by adopting a predictor network to enforce predictability on the embeddings.</p>
                  </td>
                  						   </tr>



                               <tr>
                  <td width="25%">
                    <img src="./pdf/aaai22.png" height="157" width="157">
                    </td>
                  <td width="75%" valign="top">
                    <p>
					<a href="https://arxiv.org/abs/2209.04599"><papertitle>
Preserving Privacy in Federated Learning with Ensemble Cross-Domain Knowledge Distillation</papertitle></a> <br>
					<strong>Xuan Gong</strong>, 
					<a href="https://www.linkedin.com/in/abhi5heksharma">Abhishek Sharma</a>, 
					<a href="https://karanams.github.io/">Srikrishna Karanam</a>,   
					<a href="http://wuziyan.com/">Ziyan Wu</a>,  
					<a href="https://scholar.google.com/citations?user=S2BT6ogAAAAJ&hl=en">Terrence Chen</a>, 
					<a href="https://cse.buffalo.edu/~doermann/">David Doermann</a>,
					<a href="https://www.linkedin.com/in/arun-innanje">Arun Innanje</a>
					<br><em>AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>)</em>, 
					2022 <br>
                    &nbsp;[<a href="https://github.com/gong-xuan/FedKD">code</a>]<br>
                    </p>
                    <p>We propose a quantized and noisy ensemble of local predictions from completely trained local models for stronger privacy guarantees without sacrificing accuracy. Based on extensive experiments on classification and segmentation tasks, we show that our method outperforms baseline FL algorithms with superior performance in both accuracy and data privacy preservation.
 </p>
                  </td>
                  </tr>
                
                <tr>
                  <td width="25%">
                    <img src="./pdf/FedDA.png" height="157" width="157">
                    </td>
                  <td width="75%" valign="top">
                    <p>
					<a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Gong_Ensemble_Attention_Distillation_for_Privacy-Preserving_Federated_Learning_ICCV_2021_paper.pdf"><papertitle>
Ensemble Attention Distillation for Privacy-Preserving Federated Learning</papertitle></a> <br>
					<strong>Xuan Gong</strong>, 
					<a href="https://www.linkedin.com/in/abhi5heksharma">Abhishek Sharma</a>, 
					<a href="https://karanams.github.io/">Srikrishna Karanam</a>,   
					<a href="http://wuziyan.com/">Ziyan Wu</a>,  
					<a href="https://scholar.google.com/citations?user=S2BT6ogAAAAJ&hl=en">Terrence Chen</a>, 
					<a href="https://cse.buffalo.edu/~doermann/">David Doermann</a>,
					<a href="https://www.linkedin.com/in/arun-innanje">Arun Innanje</a>
					<br><em>IEEE/CVF International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2021 <br>
					</p>
                    <p>We propose a new distillation-based FL framework that can 
					preserve privacy by design, while also consuming 
					substantially less network communication resources when 
					compared to the current methods. Our framework engages in 
					inter-node communication using only publicly available and 
					approved datasets, thereby giving explicit privacy control 
					to the user. To distill knowledge among the various local 
					models, our framework involves a novel ensemble distillation 
					algorithm that uses both final prediction as well as model 
					attention.
 </p>
                  </td>
                  </tr>


              </tbody>
            </table>
            
    <p class="auto-style2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	<a href="https://jonbarron.info/">Template Credit&nbsp;&nbsp; </a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	</p>
  </body>
</html>
